{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational autoencoder is a specific type of neural network that helps to generate complex models based on data sets. In general, autoencoders are often talked about as a type of deep learning network that tries to reconstruct a model or match the target outputs to provided inputs through the principle of backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪心搜索最为简单，直接选择每个输出的最大概率，直到出现终结符或最大句子长度。\n",
    "集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。集束搜索属于贪心算法，不能保证一定能够找到全局最优解，因为考虑到搜索空间太大，而采用一个相对的较优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从原句子X翻译目标句子Y时，句子X中任意单词对生成某个目标单词yi来说影响力都是不同的，就是所谓的注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无法理解语义，也无法解决一词多义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个词语的表征都是整个输入语句的函数。\n",
    "具体做法就是先在大语料上以language model为目标训练出bidirectional LSTM模型，\n",
    "然后利用LSTM产生词语的表征。ELMo故而得名(Embeddings from Language Models)。\n",
    "而ELMo的主要做法是先训练一个完整的语言模型，再用这个语言模型去处理需要训练的文本，生成相应的词向量，所以在文中一直强调ELMo的模型对同一个字在不同句子中能生成不同的词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从语义特征提取能力来说，目前实验支持如下结论：Transformer在这方面的能力非常显著地超过RNN和CNN（在考察语义类能力的任务WSD中，Transformer超过RNN和CNN大约4-8个绝对百分点），RNN和CNN两者能力差不太多。\n",
    "在长距离特征捕获能力方面，目前在特定的长距离特征捕获能力测试任务中（主语-谓语一致性检测，比如we……..are…），实验支持如下结论：原生CNN特征抽取器在这方面极为显著地弱于RNN和Transformer，Transformer微弱优于RNN模型(尤其在主语谓语距离小于13时)，能力由强到弱排序为Transformer>RNN>>CNN; 但在比较远的距离上（主语谓语距离大于13），RNN微弱优于Transformer，所以综合看，可以认为Transformer和RNN在这方面能力差不太多，而CNN则显著弱于前两者。\n",
    "在两个机器翻译任务中，可以看到，翻译质量指标BLEU证明了如下结论：Transformer综合能力要明显强于RNN和CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一层网络输入（上一层网络的输出）的统计特性是随着网络层数的不同而在变化的，因为每一层网络的参数是不一样的，所以我们不能简单的找到一个固定的mean和std去逐层网络的做Batch Normalization。\n",
    "所以要使用层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在Transformer中使用Self-Attention的目的是用它来代替RNN。RNN只能关注到过去的信息，而Self-Attention通过矩阵运算可以同时关注到当前时刻的上下文中所有的信息，这使得其可以实现和RNN等价甚至更好的效果。同时，RNN作为一种串行的序列模型还有一个很重要的特征，就是它能够考虑到单词的顺序(位置)关系。在同一个句子中，即使所有的词都相同但词序的变化也可能导致句子的语义完全不同，比如”北京到上海的机票”与”上海到北京的机票”，它们的语义就有很大的差别。而Self-Attention结构是不考虑词的顺序的，如果不引入位置信息，前一个例子两句话中中的\"北京\"会被编码成相同的向量，但实际上我们希望两者的编码向量是不同的，前一句中的\"北京\"需要编码出发城市的语义，而后一句中的\"北京\"则为目的城市。换言之，如果没有位置信息，Self-Attention只是一个结构更复杂的词袋模型。所以，在词向量编码中引入位置信息是必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention的核心在于学习序列中其他部分对于该部分的权重值。如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互；另一种方法是使用全连接网络。全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。这时，就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（Self-Attention Model）。\n",
    "Multihead attention结构的主要作用在于：提高了模型的表达能力，一组Q，K，V可以只捕捉句中单词的一组相关关系，也就是“表达子空间”，多组Q，K，V则可以捕捉句中单词的多组相关关系。多头注意力(Multi-head Attention)是利用多个查询来平行计算从输入信息中选取多组信息。每个注意力关注输入信息的不同部分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT采取预训练+FineTuning两个阶段，它采取Transformer作为特征抽取器。\n",
    "预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。\n",
    "第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Textual entailment: For entailment tasks, we concatenate the premise p and hypothesis h token\n",
    "sequences, with a delimiter token ($) in between.\n",
    "2. Similarity: For similarity tasks, there is no inherent ordering of the two sentences being compared.\n",
    "To reﬂect this, we modify the input sequence to contain both possible sentence orderings (with a\n",
    "delimiter in between) and process each independently to produce two sequence representations hm l\n",
    "which are added element-wise before being fed into the linear output layer.\n",
    "3. Question Answering and Commonsense Reasoning. For these tasks, we are given a context\n",
    "document z, a question q, and a set of possible answers fakg. We concatenate the document context\n",
    "and question with each possible answer, adding a delimiter token in between to get [z; q; $; ak]. Each\n",
    "of these sequences are processed independently with our model and then normalized via a softmax\n",
    "layer to produce an output distribution over possible answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a deep bidirectional representa-\n",
    "tion, we simply mask some percentage of the input\n",
    "tokens at random, and then predict those masked\n",
    "tokens. We refer to this procedure as a “masked\n",
    "LM” (MLM), although it is often referred to as a\n",
    "Cloze task in the literature (Taylor, 1953). In this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input embeddings are the sum of the token embeddings, the segmenta-\n",
    "tion embeddings and the position embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 句子对分类任务\n",
    "\n",
    "1.1 MNLI\n",
    "Williams等人提出的多体自然语言推理（Multi-Genre Natural Language Inference）是一项大规模的分类任务。给定一对句子，目标是预测第二个句子相对于第一个句子是包含，矛盾还是中立的。\n",
    "\n",
    "1.2 QQP\n",
    "Chen等人提出的Quora Question Pairs是一个二分类任务，目标是确定在Quora上询问的两个问题在语义上是否等效。\n",
    "\n",
    "1.3 QNLI\n",
    "Wang等人出的Question Natural Language Inference是Stanford Question Answering数据集[25]的一个版本，该数据集已转换为二分类任务。正例是（问题，句子）对，它们确实包含正确答案，而负例是同一段中的（问题，句子），不包含答案。\n",
    "\n",
    "1.4 STS-B\n",
    "Cer等人提出的语义文本相似性基准（The Semantic Textual Similarity Benchmark）是从新闻头条和其他来源提取的句子对的集合。它们用1到5的分数来标注，表示这两个句子在语义上有多相似。\n",
    "\n",
    "1.5 MRPC\n",
    "Dolan等人提出的Microsoft Research Paraphrase Corpus由自动从在线新闻源中提取的句子对组成，并带有人工标注，以说明句子对中的句子在语义上是否等效。\n",
    "\n",
    "1.6 RTE\n",
    "Bentivogli等人提出的识别文本蕴含（Recognizing Textual Entailment）是类似于MNLI的二进制蕴含任务，但是训练数据少得多。\n",
    "\n",
    "1.7 SWAG\n",
    "Zellers等人提出的对抗生成的情境（Situations With Adversarial Generations）数据集包含113k个句子对完整示例，用于评估扎实的常识推理。给定一个句子，任务是在四个选择中选择最合理的连续性。其中，在SWAG数据集上进行微调时，我们根据如下操作构造训练数据：每个输入序列都包含给定句子（句子A）和可能的延续词（句子B）的串联。\n",
    "\n",
    "\n",
    "2 单句子分类任务\n",
    "\n",
    "2.1 SST-2\n",
    "Socher等人提出的斯坦福情感树库（Stanford Sentiment Treebank）是一种单句二分类任务，包括从电影评论中提取的句子以及带有其情绪的人类标注。\n",
    "\n",
    "2.2 CoLA\n",
    "Warstadt等人提出的语言可接受性语料库（Corpus of Linguistic Acceptability）也是一个单句二分类任务，目标是预测英语句子在语言上是否“可以接受”。\n",
    "单句子分类任务可以直接在预训练模型中添加了一个简单的分类层，而后便可在下游任务上共同对所有参数进行微调了。\n",
    "\n",
    "3 问答任务\n",
    "SQuAD v1.1：Rajpurkar等人提出的斯坦福问答数据集（Stanford Question Answering Dataset）是10万个问题/答案对的集合。给定一个问题以及Wikipedia中包含答案的段落，任务是预测段落中的答案文本范围（start，end）。\n",
    "\n",
    "4 单句子标注任务\n",
    "单句子标注任务也叫命名实体识别任务（Named Entity Recognition），简称NER，常见的NER数据集有CoNLL-2003 NER等。该任务是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等，以及时间、数量、货币、比例数值等文字。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT，即generative pre-train model，采取预训练+FineTuning两个阶段，它采取Transformer作为特征抽取器。预训练阶段采用“单向语言模型”作为训练任务，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。\n",
    "BERT 预训练采用双向语言模型。\n",
    "本质上，GPT2.0选择了这么一条路来强化Bert或者是强化GPT 1.0的第一个预训练阶段：就是说首先把Transformer模型参数扩容，常规的Transformer Big包含24个叠加的Block，GPT 2.0包含48层，高了一倍，参数规模15亿，GPT2.0找了800万互联网网页作为语言模型的训练数据，它们被称为WebText。当然，光量大还不够，互联网网页还有个好处，覆盖的主题范围非常广，这样训练出来的语言模型，通用性好，覆盖几乎任何领域的内容，这意味着它可以用于任意领域的下游任务。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
