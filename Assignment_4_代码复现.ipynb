{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inputs=[]):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        for n in self.inputs:\n",
    "            n.outputs.append(self)\n",
    "            # set 'self' node as inbound_nodes's outbound_nodes\n",
    "        \n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients={}\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        #\\partial{node}{input_i}\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation.\n",
    "        Compute the output value based on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "        raise NotImplemented\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "    \n",
    "    def forward(self,value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other nodes implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        Example:\n",
    "        va10: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        if value is not None:\n",
    "            self.value=value\n",
    "            ## It's is the input node, when need to forward, this node \n",
    "            ## initiate self's value.\n",
    "            \n",
    "        #Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {self:0} # initialization\n",
    "        for n in self.outputs:\n",
    "            grad_cost=n.gradients[self]\n",
    "            self.gradients[self]= grad_cost * 1\n",
    "            \n",
    "        # input N --> N1,N2\n",
    "        # \\partial L / \\partial N\n",
    "        # ==> \\partial L / \\ partial N1 * \\ partial N1 / \\partial N\n",
    "        \n",
    "class Add(Node):\n",
    "    def __init__(self,*node):\n",
    "        Node._init_(self,nodes)\n",
    "        \n",
    "    def forward(self):\n",
    "        slf.value=sum(map(lambda n: n.value,self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "        \n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C        \n",
    "        \n",
    "            \n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 158.334\n",
      "Epoch: 101, Loss: 5.629\n",
      "Epoch: 201, Loss: 4.894\n",
      "Epoch: 301, Loss: 3.918\n",
      "Epoch: 401, Loss: 4.249\n",
      "Epoch: 501, Loss: 3.559\n",
      "Epoch: 601, Loss: 3.633\n",
      "Epoch: 701, Loss: 3.972\n",
      "Epoch: 801, Loss: 3.751\n",
      "Epoch: 901, Loss: 2.881\n",
      "Epoch: 1001, Loss: 3.578\n",
      "Epoch: 1101, Loss: 3.143\n",
      "Epoch: 1201, Loss: 3.193\n",
      "Epoch: 1301, Loss: 3.737\n",
      "Epoch: 1401, Loss: 3.131\n",
      "Epoch: 1501, Loss: 3.190\n",
      "Epoch: 1601, Loss: 3.503\n",
      "Epoch: 1701, Loss: 3.311\n",
      "Epoch: 1801, Loss: 3.327\n",
      "Epoch: 1901, Loss: 3.367\n",
      "Epoch: 2001, Loss: 2.848\n",
      "Epoch: 2101, Loss: 3.886\n",
      "Epoch: 2201, Loss: 3.843\n",
      "Epoch: 2301, Loss: 3.026\n",
      "Epoch: 2401, Loss: 3.550\n",
      "Epoch: 2501, Loss: 2.837\n",
      "Epoch: 2601, Loss: 2.913\n",
      "Epoch: 2701, Loss: 3.123\n",
      "Epoch: 2801, Loss: 2.951\n",
      "Epoch: 2901, Loss: 3.040\n",
      "Epoch: 3001, Loss: 3.056\n",
      "Epoch: 3101, Loss: 2.830\n",
      "Epoch: 3201, Loss: 2.566\n",
      "Epoch: 3301, Loss: 2.917\n",
      "Epoch: 3401, Loss: 3.043\n",
      "Epoch: 3501, Loss: 3.548\n",
      "Epoch: 3601, Loss: 3.452\n",
      "Epoch: 3701, Loss: 2.372\n",
      "Epoch: 3801, Loss: 2.785\n",
      "Epoch: 3901, Loss: 3.248\n",
      "Epoch: 4001, Loss: 2.832\n",
      "Epoch: 4101, Loss: 2.698\n",
      "Epoch: 4201, Loss: 3.064\n",
      "Epoch: 4301, Loss: 3.194\n",
      "Epoch: 4401, Loss: 2.751\n",
      "Epoch: 4501, Loss: 3.186\n",
      "Epoch: 4601, Loss: 2.427\n",
      "Epoch: 4701, Loss: 2.978\n",
      "Epoch: 4801, Loss: 3.053\n",
      "Epoch: 4901, Loss: 2.861\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.00126385],\n",
       "       [20.04825774],\n",
       "       [21.56174855],\n",
       "       [28.21346652],\n",
       "       [35.52771533],\n",
       "       [34.02598432],\n",
       "       [27.75469189],\n",
       "       [22.16418034],\n",
       "       [16.5137296 ],\n",
       "       [19.33326527],\n",
       "       [22.19478324],\n",
       "       [18.86526944],\n",
       "       [19.66234174],\n",
       "       [23.67735045],\n",
       "       [21.20817456],\n",
       "       [ 8.43480746]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x8fa4048>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)),losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.50172922],\n",
       "       [ 7.89559892],\n",
       "       [21.98510882],\n",
       "       [ 6.12256685],\n",
       "       [ 8.12591668],\n",
       "       [ 4.96416275],\n",
       "       [ 5.0883343 ],\n",
       "       [ 6.10847279],\n",
       "       [ 4.67252841],\n",
       "       [13.61401602]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_=data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "506/506 [==============================] - 0s 915us/step - loss: 156.7410 - mse: 156.7410\n",
      "Epoch 2/5000\n",
      "506/506 [==============================] - 0s 310us/step - loss: 82.0108 - mse: 82.0108\n",
      "Epoch 3/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 80.9755 - mse: 80.9755\n",
      "Epoch 4/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 81.6111 - mse: 81.6112\n",
      "Epoch 5/5000\n",
      "506/506 [==============================] - 0s 204us/step - loss: 85.0044 - mse: 85.0044\n",
      "Epoch 6/5000\n",
      "506/506 [==============================] - 0s 206us/step - loss: 82.7118 - mse: 82.7118\n",
      "Epoch 7/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 85.9300 - mse: 85.9300\n",
      "Epoch 8/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 83.5978 - mse: 83.5978\n",
      "Epoch 9/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 83.6346 - mse: 83.6346\n",
      "Epoch 10/5000\n",
      "506/506 [==============================] - 0s 170us/step - loss: 84.6123 - mse: 84.6123\n",
      "Epoch 11/5000\n",
      "506/506 [==============================] - 0s 275us/step - loss: 83.9524 - mse: 83.9524 0s - loss: 75.8928 - mse: 75.892\n",
      "Epoch 12/5000\n",
      "506/506 [==============================] - 0s 253us/step - loss: 83.9806 - mse: 83.9806\n",
      "Epoch 13/5000\n",
      "506/506 [==============================] - 0s 174us/step - loss: 83.1981 - mse: 83.1982\n",
      "Epoch 14/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 82.5436 - mse: 82.5436\n",
      "Epoch 15/5000\n",
      "506/506 [==============================] - 0s 200us/step - loss: 81.5036 - mse: 81.5036\n",
      "Epoch 16/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 82.0044 - mse: 82.0045\n",
      "Epoch 17/5000\n",
      "506/506 [==============================] - 0s 190us/step - loss: 81.6497 - mse: 81.6497\n",
      "Epoch 18/5000\n",
      "506/506 [==============================] - 0s 168us/step - loss: 81.9605 - mse: 81.9604\n",
      "Epoch 19/5000\n",
      "506/506 [==============================] - 0s 265us/step - loss: 81.3025 - mse: 81.3025\n",
      "Epoch 20/5000\n",
      "506/506 [==============================] - 0s 328us/step - loss: 81.9702 - mse: 81.9702\n",
      "Epoch 21/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 82.4423 - mse: 82.4423\n",
      "Epoch 22/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 79.5598 - mse: 79.5598\n",
      "Epoch 23/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 79.2394 - mse: 79.2395\n",
      "Epoch 24/5000\n",
      "506/506 [==============================] - 0s 296us/step - loss: 81.2647 - mse: 81.2647\n",
      "Epoch 25/5000\n",
      "506/506 [==============================] - 0s 383us/step - loss: 80.0566 - mse: 80.0566\n",
      "Epoch 26/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 83.1197 - mse: 83.1197\n",
      "Epoch 27/5000\n",
      "506/506 [==============================] - 0s 204us/step - loss: 80.3993 - mse: 80.3993\n",
      "Epoch 28/5000\n",
      "506/506 [==============================] - 0s 198us/step - loss: 78.8410 - mse: 78.8410\n",
      "Epoch 29/5000\n",
      "506/506 [==============================] - 0s 184us/step - loss: 81.3367 - mse: 81.3367\n",
      "Epoch 30/5000\n",
      "506/506 [==============================] - 0s 206us/step - loss: 80.0861 - mse: 80.0861\n",
      "Epoch 31/5000\n",
      "506/506 [==============================] - 0s 247us/step - loss: 80.1633 - mse: 80.1633\n",
      "Epoch 32/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 78.5780 - mse: 78.5780\n",
      "Epoch 33/5000\n",
      "506/506 [==============================] - 0s 332us/step - loss: 81.1222 - mse: 81.1222\n",
      "Epoch 34/5000\n",
      "506/506 [==============================] - 0s 213us/step - loss: 82.6589 - mse: 82.6589\n",
      "Epoch 35/5000\n",
      "506/506 [==============================] - 0s 225us/step - loss: 81.3166 - mse: 81.3166\n",
      "Epoch 36/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 80.1686 - mse: 80.1686\n",
      "Epoch 37/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 78.5201 - mse: 78.5201\n",
      "Epoch 38/5000\n",
      "506/506 [==============================] - 0s 204us/step - loss: 80.0086 - mse: 80.0086\n",
      "Epoch 39/5000\n",
      "506/506 [==============================] - 0s 745us/step - loss: 79.6941 - mse: 79.6941\n",
      "Epoch 40/5000\n",
      "506/506 [==============================] - 0s 245us/step - loss: 78.2491 - mse: 78.2491\n",
      "Epoch 41/5000\n",
      "506/506 [==============================] - 0s 206us/step - loss: 81.1166 - mse: 81.1166\n",
      "Epoch 42/5000\n",
      "506/506 [==============================] - 0s 235us/step - loss: 79.9473 - mse: 79.9473\n",
      "Epoch 43/5000\n",
      "506/506 [==============================] - 0s 348us/step - loss: 78.4122 - mse: 78.4122\n",
      "Epoch 44/5000\n",
      "506/506 [==============================] - 0s 397us/step - loss: 79.3603 - mse: 79.3603 0s - loss: 67.5325 - mse: 67.5\n",
      "Epoch 45/5000\n",
      "506/506 [==============================] - 0s 385us/step - loss: 79.3885 - mse: 79.3885\n",
      "Epoch 46/5000\n",
      "506/506 [==============================] - 0s 287us/step - loss: 78.1024 - mse: 78.1024\n",
      "Epoch 47/5000\n",
      "506/506 [==============================] - 0s 698us/step - loss: 77.7656 - mse: 77.7656\n",
      "Epoch 48/5000\n",
      "506/506 [==============================] - 0s 820us/step - loss: 78.3175 - mse: 78.3175\n",
      "Epoch 49/5000\n",
      "506/506 [==============================] - 0s 775us/step - loss: 77.9662 - mse: 77.9662\n",
      "Epoch 50/5000\n",
      "506/506 [==============================] - 0s 690us/step - loss: 79.2572 - mse: 79.2572\n",
      "Epoch 51/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 77.7781 - mse: 77.7781\n",
      "Epoch 52/5000\n",
      "506/506 [==============================] - 0s 642us/step - loss: 78.8434 - mse: 78.8434\n",
      "Epoch 53/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 79.1663 - mse: 79.1664\n",
      "Epoch 54/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 78.5062 - mse: 78.5062\n",
      "Epoch 55/5000\n",
      "506/506 [==============================] - 1s 998us/step - loss: 78.1665 - mse: 78.1665\n",
      "Epoch 56/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 77.1214 - mse: 77.1214A: 0s - loss: 87.3062 - mse: \n",
      "Epoch 57/5000\n",
      "506/506 [==============================] - 0s 846us/step - loss: 78.1514 - mse: 78.1514\n",
      "Epoch 58/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 79.3411 - mse: 79.3411\n",
      "Epoch 59/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 80.0338 - mse: 80.0338\n",
      "Epoch 60/5000\n",
      "506/506 [==============================] - 0s 970us/step - loss: 79.5403 - mse: 79.5403\n",
      "Epoch 61/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 78.6070 - mse: 78.6070\n",
      "Epoch 62/5000\n",
      "506/506 [==============================] - 0s 824us/step - loss: 79.0958 - mse: 79.0958\n",
      "Epoch 63/5000\n",
      "506/506 [==============================] - 0s 968us/step - loss: 78.4435 - mse: 78.4435\n",
      "Epoch 64/5000\n",
      "506/506 [==============================] - 0s 840us/step - loss: 78.3562 - mse: 78.3562\n",
      "Epoch 65/5000\n",
      "506/506 [==============================] - 0s 753us/step - loss: 78.3307 - mse: 78.3307\n",
      "Epoch 66/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 77.3059 - mse: 77.3059\n",
      "Epoch 67/5000\n",
      "506/506 [==============================] - 0s 781us/step - loss: 77.2120 - mse: 77.2120\n",
      "Epoch 68/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 78.8670 - mse: 78.8670\n",
      "Epoch 69/5000\n",
      "506/506 [==============================] - 0s 700us/step - loss: 78.4483 - mse: 78.4483\n",
      "Epoch 70/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 78.5115 - mse: 78.5115\n",
      "Epoch 71/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 77.9079 - mse: 77.9080\n",
      "Epoch 72/5000\n",
      "506/506 [==============================] - 0s 970us/step - loss: 77.7225 - mse: 77.7225 0s - loss: 78.1415 - mse: 78.1\n",
      "Epoch 73/5000\n",
      "506/506 [==============================] - 0s 878us/step - loss: 78.2785 - mse: 78.2785\n",
      "Epoch 74/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 77.2804 - mse: 77.2804\n",
      "Epoch 75/5000\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 77.4739 - mse: 77.4739\n",
      "Epoch 76/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 78.2701 - mse: 78.2701\n",
      "Epoch 77/5000\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 78.9654 - mse: 78.9654\n",
      "Epoch 78/5000\n",
      "506/506 [==============================] - 0s 838us/step - loss: 77.9872 - mse: 77.9872 0s - loss: 79.8326 - mse: 79.832\n",
      "Epoch 79/5000\n",
      "506/506 [==============================] - 0s 300us/step - loss: 78.9288 - mse: 78.9288\n",
      "Epoch 80/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 516us/step - loss: 77.6653 - mse: 77.6653\n",
      "Epoch 81/5000\n",
      "506/506 [==============================] - 0s 269us/step - loss: 77.8771 - mse: 77.8771\n",
      "Epoch 82/5000\n",
      "506/506 [==============================] - 0s 285us/step - loss: 78.6037 - mse: 78.6037\n",
      "Epoch 83/5000\n",
      "506/506 [==============================] - 0s 251us/step - loss: 83.9400 - mse: 83.9400A: 0s - loss: 70.4088 - mse: 70.4088\n",
      "Epoch 84/5000\n",
      "506/506 [==============================] - 0s 223us/step - loss: 86.2535 - mse: 86.2535\n",
      "Epoch 85/5000\n",
      "506/506 [==============================] - 0s 204us/step - loss: 85.9040 - mse: 85.9040\n",
      "Epoch 86/5000\n",
      "506/506 [==============================] - 0s 356us/step - loss: 84.6735 - mse: 84.6735\n",
      "Epoch 87/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 85.0787 - mse: 85.0787\n",
      "Epoch 88/5000\n",
      "506/506 [==============================] - 0s 289us/step - loss: 85.3281 - mse: 85.3281\n",
      "Epoch 89/5000\n",
      "506/506 [==============================] - 0s 231us/step - loss: 85.1204 - mse: 85.1204\n",
      "Epoch 90/5000\n",
      "506/506 [==============================] - 0s 437us/step - loss: 85.2489 - mse: 85.2489\n",
      "Epoch 91/5000\n",
      "506/506 [==============================] - 0s 172us/step - loss: 84.6861 - mse: 84.6861\n",
      "Epoch 92/5000\n",
      "506/506 [==============================] - 0s 338us/step - loss: 85.4893 - mse: 85.4893\n",
      "Epoch 93/5000\n",
      "506/506 [==============================] - 0s 239us/step - loss: 85.2047 - mse: 85.2047\n",
      "Epoch 94/5000\n",
      "506/506 [==============================] - 0s 551us/step - loss: 85.0556 - mse: 85.0556\n",
      "Epoch 95/5000\n",
      "506/506 [==============================] - 0s 332us/step - loss: 85.0618 - mse: 85.0618\n",
      "Epoch 96/5000\n",
      "506/506 [==============================] - 0s 255us/step - loss: 85.7510 - mse: 85.7510\n",
      "Epoch 97/5000\n",
      "506/506 [==============================] - 0s 316us/step - loss: 85.3470 - mse: 85.3470\n",
      "Epoch 98/5000\n",
      "506/506 [==============================] - 0s 413us/step - loss: 85.3633 - mse: 85.3633\n",
      "Epoch 99/5000\n",
      "506/506 [==============================] - 0s 316us/step - loss: 84.9782 - mse: 84.9782\n",
      "Epoch 100/5000\n",
      "506/506 [==============================] - 0s 269us/step - loss: 85.2563 - mse: 85.2563\n",
      "Epoch 101/5000\n",
      "506/506 [==============================] - 0s 255us/step - loss: 84.7894 - mse: 84.7894\n",
      "Epoch 102/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 87.2277 - mse: 87.2277 - 0s 310us/step - loss: 85.0165 - mse: 85.0165\n",
      "Epoch 103/5000\n",
      "506/506 [==============================] - 0s 409us/step - loss: 84.5084 - mse: 84.5084\n",
      "Epoch 104/5000\n",
      "506/506 [==============================] - 0s 279us/step - loss: 85.0757 - mse: 85.0757\n",
      "Epoch 105/5000\n",
      "506/506 [==============================] - 0s 188us/step - loss: 84.8980 - mse: 84.8980\n",
      "Epoch 106/5000\n",
      "506/506 [==============================] - 0s 346us/step - loss: 84.6111 - mse: 84.6111\n",
      "Epoch 107/5000\n",
      "506/506 [==============================] - 0s 336us/step - loss: 85.2966 - mse: 85.2966\n",
      "Epoch 108/5000\n",
      "506/506 [==============================] - 0s 381us/step - loss: 84.6705 - mse: 84.6705\n",
      "Epoch 109/5000\n",
      "506/506 [==============================] - 0s 328us/step - loss: 85.4685 - mse: 85.4685\n",
      "Epoch 110/5000\n",
      "506/506 [==============================] - 0s 188us/step - loss: 85.0818 - mse: 85.0818\n",
      "Epoch 111/5000\n",
      "506/506 [==============================] - 0s 352us/step - loss: 85.0391 - mse: 85.0392\n",
      "Epoch 112/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 84.8999 - mse: 84.8999\n",
      "Epoch 113/5000\n",
      "506/506 [==============================] - 0s 285us/step - loss: 84.7420 - mse: 84.7420\n",
      "Epoch 114/5000\n",
      "506/506 [==============================] - 0s 664us/step - loss: 85.3491 - mse: 85.3491\n",
      "Epoch 115/5000\n",
      "506/506 [==============================] - 0s 287us/step - loss: 84.6725 - mse: 84.6725\n",
      "Epoch 116/5000\n",
      "506/506 [==============================] - 0s 563us/step - loss: 84.7933 - mse: 84.7933\n",
      "Epoch 117/5000\n",
      "506/506 [==============================] - 0s 330us/step - loss: 85.0697 - mse: 85.0697\n",
      "Epoch 118/5000\n",
      "506/506 [==============================] - 0s 443us/step - loss: 85.1027 - mse: 85.1027\n",
      "Epoch 119/5000\n",
      "506/506 [==============================] - 0s 480us/step - loss: 85.1357 - mse: 85.1357\n",
      "Epoch 120/5000\n",
      "506/506 [==============================] - 0s 405us/step - loss: 84.5571 - mse: 84.5571\n",
      "Epoch 121/5000\n",
      "506/506 [==============================] - 0s 176us/step - loss: 85.1308 - mse: 85.1308\n",
      "Epoch 122/5000\n",
      "506/506 [==============================] - 0s 542us/step - loss: 84.7540 - mse: 84.7540\n",
      "Epoch 123/5000\n",
      "506/506 [==============================] - 0s 464us/step - loss: 85.0834 - mse: 85.0834\n",
      "Epoch 124/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 84.5315 - mse: 84.5315\n",
      "Epoch 125/5000\n",
      "506/506 [==============================] - 0s 221us/step - loss: 84.8143 - mse: 84.8143\n",
      "Epoch 126/5000\n",
      "506/506 [==============================] - 0s 293us/step - loss: 85.0169 - mse: 85.0169\n",
      "Epoch 127/5000\n",
      "506/506 [==============================] - 0s 506us/step - loss: 84.6964 - mse: 84.6964A: 0s - loss: 97.2205 - mse: 97.22\n",
      "Epoch 128/5000\n",
      "506/506 [==============================] - 0s 587us/step - loss: 85.1974 - mse: 85.1974\n",
      "Epoch 129/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 85.3793 - mse: 85.3793\n",
      "Epoch 130/5000\n",
      "506/506 [==============================] - 0s 360us/step - loss: 85.3124 - mse: 85.3124\n",
      "Epoch 131/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 84.8400 - mse: 84.8400\n",
      "Epoch 132/5000\n",
      "506/506 [==============================] - 0s 407us/step - loss: 85.7734 - mse: 85.7734\n",
      "Epoch 133/5000\n",
      "506/506 [==============================] - 0s 198us/step - loss: 85.1611 - mse: 85.1611 0s - loss: 85.4400 - mse: 85.440\n",
      "Epoch 134/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.7130 - mse: 84.7130\n",
      "Epoch 135/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.3920 - mse: 84.3920\n",
      "Epoch 136/5000\n",
      "506/506 [==============================] - 0s 972us/step - loss: 85.0529 - mse: 85.0529\n",
      "Epoch 137/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.2205 - mse: 85.2205\n",
      "Epoch 138/5000\n",
      "506/506 [==============================] - 0s 247us/step - loss: 85.0974 - mse: 85.0974\n",
      "Epoch 139/5000\n",
      "506/506 [==============================] - 0s 435us/step - loss: 84.7228 - mse: 84.7228\n",
      "Epoch 140/5000\n",
      "506/506 [==============================] - 0s 494us/step - loss: 84.7742 - mse: 84.7742\n",
      "Epoch 141/5000\n",
      "506/506 [==============================] - 0s 379us/step - loss: 85.2041 - mse: 85.2041\n",
      "Epoch 142/5000\n",
      "506/506 [==============================] - 0s 723us/step - loss: 83.9826 - mse: 83.9826\n",
      "Epoch 143/5000\n",
      "506/506 [==============================] - 0s 526us/step - loss: 85.4075 - mse: 85.4075\n",
      "Epoch 144/5000\n",
      "506/506 [==============================] - 0s 559us/step - loss: 85.3632 - mse: 85.3632\n",
      "Epoch 145/5000\n",
      "506/506 [==============================] - 0s 117us/step - loss: 85.4229 - mse: 85.4230\n",
      "Epoch 146/5000\n",
      "506/506 [==============================] - 0s 621us/step - loss: 84.9472 - mse: 84.9472\n",
      "Epoch 147/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8043 - mse: 84.8043\n",
      "Epoch 148/5000\n",
      "506/506 [==============================] - 0s 945us/step - loss: 85.2426 - mse: 85.2426\n",
      "Epoch 149/5000\n",
      "506/506 [==============================] - 0s 911us/step - loss: 84.7025 - mse: 84.7025\n",
      "Epoch 150/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.0814 - mse: 85.0814\n",
      "Epoch 151/5000\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 84.9199 - mse: 84.9199\n",
      "Epoch 152/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.2364 - mse: 85.2364\n",
      "Epoch 153/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.5382 - mse: 84.5382\n",
      "Epoch 154/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8891 - mse: 84.8891\n",
      "Epoch 155/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8691 - mse: 84.8691\n",
      "Epoch 156/5000\n",
      "506/506 [==============================] - 0s 360us/step - loss: 84.7869 - mse: 84.7869\n",
      "Epoch 157/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 84.9049 - mse: 84.9049\n",
      "Epoch 158/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 245us/step - loss: 84.9340 - mse: 84.9340\n",
      "Epoch 159/5000\n",
      "506/506 [==============================] - 0s 267us/step - loss: 84.5298 - mse: 84.5298\n",
      "Epoch 160/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 84.9384 - mse: 84.9384\n",
      "Epoch 161/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 84.8187 - mse: 84.8187\n",
      "Epoch 162/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 84.6082 - mse: 84.6082\n",
      "Epoch 163/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 85.1761 - mse: 85.1761\n",
      "Epoch 164/5000\n",
      "506/506 [==============================] - 0s 277us/step - loss: 85.2290 - mse: 85.2290A: 0s - loss: 90.6482 - mse: 90.6482\n",
      "Epoch 165/5000\n",
      "506/506 [==============================] - 0s 215us/step - loss: 84.6696 - mse: 84.6695\n",
      "Epoch 166/5000\n",
      "506/506 [==============================] - 0s 188us/step - loss: 85.2769 - mse: 85.2769\n",
      "Epoch 167/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 84.8037 - mse: 84.8037\n",
      "Epoch 168/5000\n",
      "506/506 [==============================] - 0s 255us/step - loss: 85.1477 - mse: 85.1477\n",
      "Epoch 169/5000\n",
      "506/506 [==============================] - 0s 308us/step - loss: 85.1126 - mse: 85.1126\n",
      "Epoch 170/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 85.1758 - mse: 85.1758\n",
      "Epoch 171/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 75.3550 - mse: 75.355 - 0s 176us/step - loss: 85.0216 - mse: 85.0216\n",
      "Epoch 172/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 84.2797 - mse: 84.2797\n",
      "Epoch 173/5000\n",
      "506/506 [==============================] - 0s 225us/step - loss: 85.4142 - mse: 85.4142\n",
      "Epoch 174/5000\n",
      "506/506 [==============================] - 0s 287us/step - loss: 85.0645 - mse: 85.0645\n",
      "Epoch 175/5000\n",
      "506/506 [==============================] - 0s 217us/step - loss: 85.5499 - mse: 85.5499\n",
      "Epoch 176/5000\n",
      "506/506 [==============================] - 0s 215us/step - loss: 85.0282 - mse: 85.0282\n",
      "Epoch 177/5000\n",
      "506/506 [==============================] - 0s 247us/step - loss: 85.2462 - mse: 85.2462\n",
      "Epoch 178/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 85.2445 - mse: 85.2445\n",
      "Epoch 179/5000\n",
      "506/506 [==============================] - 0s 231us/step - loss: 85.6403 - mse: 85.6403\n",
      "Epoch 180/5000\n",
      "506/506 [==============================] - 0s 429us/step - loss: 84.8981 - mse: 84.8981\n",
      "Epoch 181/5000\n",
      "506/506 [==============================] - 0s 366us/step - loss: 85.0482 - mse: 85.0482\n",
      "Epoch 182/5000\n",
      "506/506 [==============================] - 0s 277us/step - loss: 84.6801 - mse: 84.6801\n",
      "Epoch 183/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 84.8715 - mse: 84.8715\n",
      "Epoch 184/5000\n",
      "506/506 [==============================] - 0s 545us/step - loss: 84.9822 - mse: 84.9822\n",
      "Epoch 185/5000\n",
      "506/506 [==============================] - 0s 257us/step - loss: 84.9121 - mse: 84.9121\n",
      "Epoch 186/5000\n",
      "506/506 [==============================] - 0s 310us/step - loss: 85.2507 - mse: 85.2507\n",
      "Epoch 187/5000\n",
      "506/506 [==============================] - 0s 257us/step - loss: 85.1763 - mse: 85.1763\n",
      "Epoch 188/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 84.7459 - mse: 84.7459\n",
      "Epoch 189/5000\n",
      "506/506 [==============================] - 0s 184us/step - loss: 84.8428 - mse: 84.8428\n",
      "Epoch 190/5000\n",
      "506/506 [==============================] - 0s 249us/step - loss: 85.0293 - mse: 85.0293\n",
      "Epoch 191/5000\n",
      "506/506 [==============================] - 0s 156us/step - loss: 85.7271 - mse: 85.7271\n",
      "Epoch 192/5000\n",
      "506/506 [==============================] - 0s 549us/step - loss: 85.2520 - mse: 85.2520\n",
      "Epoch 193/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.6664 - mse: 84.6664\n",
      "Epoch 194/5000\n",
      "506/506 [==============================] - 0s 804us/step - loss: 85.0021 - mse: 85.0021\n",
      "Epoch 195/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.1111 - mse: 85.1111\n",
      "Epoch 196/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.4530 - mse: 84.4530\n",
      "Epoch 197/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.2484 - mse: 85.2484\n",
      "Epoch 198/5000\n",
      "506/506 [==============================] - 0s 959us/step - loss: 85.2067 - mse: 85.2067\n",
      "Epoch 199/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8115 - mse: 84.8115\n",
      "Epoch 200/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8949 - mse: 84.8949\n",
      "Epoch 201/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.7540 - mse: 84.7540\n",
      "Epoch 202/5000\n",
      "506/506 [==============================] - 0s 751us/step - loss: 85.0027 - mse: 85.0027 0s - loss: 92.2598 - mse: 92.2\n",
      "Epoch 203/5000\n",
      "506/506 [==============================] - 0s 445us/step - loss: 84.7378 - mse: 84.7378\n",
      "Epoch 204/5000\n",
      "506/506 [==============================] - 0s 326us/step - loss: 85.1201 - mse: 85.1201\n",
      "Epoch 205/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 84.8821 - mse: 84.8820\n",
      "Epoch 206/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 85.0267 - mse: 85.0267\n",
      "Epoch 207/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 84.7308 - mse: 84.7308\n",
      "Epoch 208/5000\n",
      "506/506 [==============================] - 0s 287us/step - loss: 84.4951 - mse: 84.4951\n",
      "Epoch 209/5000\n",
      "506/506 [==============================] - 0s 362us/step - loss: 85.3832 - mse: 85.3832\n",
      "Epoch 210/5000\n",
      "506/506 [==============================] - 0s 490us/step - loss: 85.0198 - mse: 85.0198\n",
      "Epoch 211/5000\n",
      "506/506 [==============================] - 0s 172us/step - loss: 85.2870 - mse: 85.2871\n",
      "Epoch 212/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.5908 - mse: 84.5908\n",
      "Epoch 213/5000\n",
      "506/506 [==============================] - 0s 947us/step - loss: 85.3823 - mse: 85.3823\n",
      "Epoch 214/5000\n",
      "506/506 [==============================] - 0s 844us/step - loss: 85.6837 - mse: 85.6837\n",
      "Epoch 215/5000\n",
      "506/506 [==============================] - 0s 974us/step - loss: 84.9809 - mse: 84.9809\n",
      "Epoch 216/5000\n",
      "506/506 [==============================] - 0s 796us/step - loss: 85.2549 - mse: 85.2548\n",
      "Epoch 217/5000\n",
      "506/506 [==============================] - 0s 891us/step - loss: 84.9463 - mse: 84.9463\n",
      "Epoch 218/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.1821 - mse: 85.1821\n",
      "Epoch 219/5000\n",
      "506/506 [==============================] - 0s 557us/step - loss: 84.9273 - mse: 84.9273\n",
      "Epoch 220/5000\n",
      "506/506 [==============================] - 0s 844us/step - loss: 85.1484 - mse: 85.1484 0s - loss: 92.6692 - mse: 9\n",
      "Epoch 221/5000\n",
      "506/506 [==============================] - 0s 802us/step - loss: 84.7654 - mse: 84.7654\n",
      "Epoch 222/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.5960 - mse: 84.5960\n",
      "Epoch 223/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.4249 - mse: 85.4249\n",
      "Epoch 224/5000\n",
      "506/506 [==============================] - 0s 587us/step - loss: 85.2387 - mse: 85.2387\n",
      "Epoch 225/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8063 - mse: 84.8064\n",
      "Epoch 226/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.1758 - mse: 85.1758\n",
      "Epoch 227/5000\n",
      "506/506 [==============================] - 0s 437us/step - loss: 85.2215 - mse: 85.2215\n",
      "Epoch 228/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.6747 - mse: 84.6747\n",
      "Epoch 229/5000\n",
      "506/506 [==============================] - 0s 704us/step - loss: 85.2739 - mse: 85.2739 0s - loss: 83.2215 - mse: 83.221\n",
      "Epoch 230/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 85.1098 - mse: 85.1098A: 0s - loss: 91.8137 - mse: 91.8\n",
      "Epoch 231/5000\n",
      "506/506 [==============================] - 0s 939us/step - loss: 85.3353 - mse: 85.3353\n",
      "Epoch 232/5000\n",
      "506/506 [==============================] - 0s 763us/step - loss: 84.8658 - mse: 84.8658\n",
      "Epoch 233/5000\n",
      "506/506 [==============================] - 1s 990us/step - loss: 85.0528 - mse: 85.0528\n",
      "Epoch 234/5000\n",
      "506/506 [==============================] - 0s 798us/step - loss: 84.7026 - mse: 84.7027\n",
      "Epoch 235/5000\n",
      "506/506 [==============================] - 1s 988us/step - loss: 85.0349 - mse: 85.0349\n",
      "Epoch 236/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 838us/step - loss: 84.9088 - mse: 84.9088 0s - loss: 90.0188 - mse: 90.01\n",
      "Epoch 237/5000\n",
      "506/506 [==============================] - 0s 680us/step - loss: 85.3100 - mse: 85.3100\n",
      "Epoch 238/5000\n",
      "506/506 [==============================] - 0s 947us/step - loss: 84.8953 - mse: 84.8953\n",
      "Epoch 239/5000\n",
      "506/506 [==============================] - 0s 559us/step - loss: 84.5551 - mse: 84.5552\n",
      "Epoch 240/5000\n",
      "506/506 [==============================] - 0s 878us/step - loss: 85.0080 - mse: 85.0080\n",
      "Epoch 241/5000\n",
      "506/506 [==============================] - 0s 304us/step - loss: 84.8541 - mse: 84.8541\n",
      "Epoch 242/5000\n",
      "506/506 [==============================] - 0s 609us/step - loss: 85.1293 - mse: 85.1293\n",
      "Epoch 243/5000\n",
      "506/506 [==============================] - 0s 603us/step - loss: 85.7119 - mse: 85.7119\n",
      "Epoch 244/5000\n",
      "506/506 [==============================] - 0s 733us/step - loss: 84.8372 - mse: 84.8372\n",
      "Epoch 245/5000\n",
      "506/506 [==============================] - 0s 591us/step - loss: 85.1089 - mse: 85.1089\n",
      "Epoch 246/5000\n",
      "506/506 [==============================] - 0s 945us/step - loss: 84.7063 - mse: 84.7063\n",
      "Epoch 247/5000\n",
      "506/506 [==============================] - 0s 563us/step - loss: 85.1882 - mse: 85.1882\n",
      "Epoch 248/5000\n",
      "506/506 [==============================] - 0s 838us/step - loss: 85.2011 - mse: 85.2011\n",
      "Epoch 249/5000\n",
      "506/506 [==============================] - 0s 650us/step - loss: 85.1238 - mse: 85.1238\n",
      "Epoch 250/5000\n",
      "506/506 [==============================] - 0s 897us/step - loss: 85.0552 - mse: 85.0552\n",
      "Epoch 251/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8459 - mse: 84.8459\n",
      "Epoch 252/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 84.8937 - mse: 84.8937\n",
      "Epoch 253/5000\n",
      "506/506 [==============================] - 0s 253us/step - loss: 85.0146 - mse: 85.0146\n",
      "Epoch 254/5000\n",
      "506/506 [==============================] - 0s 263us/step - loss: 85.1095 - mse: 85.1095\n",
      "Epoch 255/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 85.2435 - mse: 85.2435\n",
      "Epoch 256/5000\n",
      "506/506 [==============================] - 0s 275us/step - loss: 84.8378 - mse: 84.8378\n",
      "Epoch 257/5000\n",
      "506/506 [==============================] - 0s 389us/step - loss: 85.1354 - mse: 85.1354\n",
      "Epoch 258/5000\n",
      "506/506 [==============================] - 0s 257us/step - loss: 84.7101 - mse: 84.7101\n",
      "Epoch 259/5000\n",
      "506/506 [==============================] - 0s 231us/step - loss: 85.2477 - mse: 85.2477\n",
      "Epoch 260/5000\n",
      "506/506 [==============================] - 0s 275us/step - loss: 84.6978 - mse: 84.6978\n",
      "Epoch 261/5000\n",
      "506/506 [==============================] - 0s 374us/step - loss: 84.9130 - mse: 84.9130\n",
      "Epoch 262/5000\n",
      "506/506 [==============================] - 0s 172us/step - loss: 84.9277 - mse: 84.9277\n",
      "Epoch 263/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 85.2855 - mse: 85.2855\n",
      "Epoch 264/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 84.3459 - mse: 84.3459\n",
      "Epoch 265/5000\n",
      "506/506 [==============================] - 0s 423us/step - loss: 84.7478 - mse: 84.7478\n",
      "Epoch 266/5000\n",
      "506/506 [==============================] - 0s 405us/step - loss: 85.1180 - mse: 85.1180\n",
      "Epoch 267/5000\n",
      "506/506 [==============================] - 0s 704us/step - loss: 84.8744 - mse: 84.8744\n",
      "Epoch 268/5000\n",
      "506/506 [==============================] - 0s 427us/step - loss: 85.3056 - mse: 85.3056\n",
      "Epoch 269/5000\n",
      "506/506 [==============================] - 0s 439us/step - loss: 85.0993 - mse: 85.0993\n",
      "Epoch 270/5000\n",
      "506/506 [==============================] - 0s 520us/step - loss: 84.7595 - mse: 84.7595\n",
      "Epoch 271/5000\n",
      "506/506 [==============================] - 0s 623us/step - loss: 84.8743 - mse: 84.8743\n",
      "Epoch 272/5000\n",
      "506/506 [==============================] - 0s 435us/step - loss: 84.9547 - mse: 84.9547\n",
      "Epoch 273/5000\n",
      "506/506 [==============================] - 0s 370us/step - loss: 84.9657 - mse: 84.9657\n",
      "Epoch 274/5000\n",
      "506/506 [==============================] - 0s 298us/step - loss: 84.6372 - mse: 84.6372 0s - loss: 82.9225 - mse: 82.92\n",
      "Epoch 275/5000\n",
      "506/506 [==============================] - 0s 510us/step - loss: 84.8382 - mse: 84.8382\n",
      "Epoch 276/5000\n",
      "506/506 [==============================] - 0s 700us/step - loss: 84.7711 - mse: 84.7711A: 0s - loss: 98.2364 - mse: 98.2\n",
      "Epoch 277/5000\n",
      "506/506 [==============================] - 0s 876us/step - loss: 84.9620 - mse: 84.9620\n",
      "Epoch 278/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.8570 - mse: 84.8569\n",
      "Epoch 279/5000\n",
      "506/506 [==============================] - 0s 913us/step - loss: 84.9841 - mse: 84.9841\n",
      "Epoch 280/5000\n",
      "506/506 [==============================] - 0s 802us/step - loss: 85.1566 - mse: 85.1566\n",
      "Epoch 281/5000\n",
      "506/506 [==============================] - 0s 747us/step - loss: 85.1140 - mse: 85.1140\n",
      "Epoch 282/5000\n",
      "506/506 [==============================] - 0s 87us/step - loss: 84.8431 - mse: 84.8431\n",
      "Epoch 283/5000\n",
      "506/506 [==============================] - 0s 391us/step - loss: 85.0090 - mse: 85.0090\n",
      "Epoch 284/5000\n",
      "506/506 [==============================] - 0s 385us/step - loss: 85.3641 - mse: 85.3641\n",
      "Epoch 285/5000\n",
      "506/506 [==============================] - 0s 544us/step - loss: 85.0279 - mse: 85.0279\n",
      "Epoch 286/5000\n",
      "506/506 [==============================] - 0s 379us/step - loss: 85.0364 - mse: 85.0364\n",
      "Epoch 287/5000\n",
      "506/506 [==============================] - 0s 316us/step - loss: 84.4272 - mse: 84.4272\n",
      "Epoch 288/5000\n",
      "506/506 [==============================] - 0s 271us/step - loss: 84.9462 - mse: 84.9462\n",
      "Epoch 289/5000\n",
      "506/506 [==============================] - 0s 850us/step - loss: 85.3102 - mse: 85.3102\n",
      "Epoch 290/5000\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 85.2041 - mse: 85.2041\n",
      "Epoch 291/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.9955 - mse: 84.9955\n",
      "Epoch 292/5000\n",
      "506/506 [==============================] - 1s 1ms/step - loss: 84.9635 - mse: 84.9635\n",
      "Epoch 293/5000\n",
      "506/506 [==============================] - 1s 2ms/step - loss: 85.2883 - mse: 85.2883\n",
      "Epoch 294/5000\n",
      "506/506 [==============================] - 0s 490us/step - loss: 85.1816 - mse: 85.1816\n",
      "Epoch 295/5000\n",
      "506/506 [==============================] - 0s 360us/step - loss: 84.5148 - mse: 84.5148\n",
      "Epoch 296/5000\n",
      "506/506 [==============================] - 0s 617us/step - loss: 85.2639 - mse: 85.2639\n",
      "Epoch 297/5000\n",
      "506/506 [==============================] - 0s 215us/step - loss: 85.4002 - mse: 85.4002\n",
      "Epoch 298/5000\n",
      "506/506 [==============================] - 0s 848us/step - loss: 85.0469 - mse: 85.0469\n",
      "Epoch 299/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 84.9651 - mse: 84.9651\n",
      "Epoch 300/5000\n",
      "506/506 [==============================] - 0s 547us/step - loss: 85.0190 - mse: 85.0190\n",
      "Epoch 301/5000\n",
      "506/506 [==============================] - 0s 433us/step - loss: 84.9178 - mse: 84.9178\n",
      "Epoch 302/5000\n",
      "506/506 [==============================] - 0s 381us/step - loss: 85.1830 - mse: 85.1830\n",
      "Epoch 303/5000\n",
      "506/506 [==============================] - 0s 457us/step - loss: 84.9036 - mse: 84.9036\n",
      "Epoch 304/5000\n",
      "506/506 [==============================] - 0s 435us/step - loss: 85.0917 - mse: 85.0917\n",
      "Epoch 305/5000\n",
      "506/506 [==============================] - 0s 650us/step - loss: 84.7677 - mse: 84.7677\n",
      "Epoch 306/5000\n",
      "506/506 [==============================] - 0s 700us/step - loss: 84.8684 - mse: 84.8684\n",
      "Epoch 307/5000\n",
      "506/506 [==============================] - 0s 498us/step - loss: 85.1856 - mse: 85.1856\n",
      "Epoch 308/5000\n",
      "506/506 [==============================] - 0s 421us/step - loss: 85.0164 - mse: 85.0164\n",
      "Epoch 309/5000\n",
      "506/506 [==============================] - 0s 486us/step - loss: 84.6292 - mse: 84.6292 0s - loss: 71.5271 - mse: 71.5\n",
      "Epoch 310/5000\n",
      "506/506 [==============================] - 0s 907us/step - loss: 84.8702 - mse: 84.8702\n",
      "Epoch 311/5000\n",
      "506/506 [==============================] - 0s 459us/step - loss: 85.0964 - mse: 85.0964\n",
      "Epoch 312/5000\n",
      "506/506 [==============================] - 0s 686us/step - loss: 84.7565 - mse: 84.7565\n",
      "Epoch 313/5000\n",
      "506/506 [==============================] - 0s 836us/step - loss: 84.7050 - mse: 84.7050\n",
      "Epoch 314/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 389us/step - loss: 84.9057 - mse: 84.9057\n",
      "Epoch 315/5000\n",
      "506/506 [==============================] - 0s 219us/step - loss: 85.0015 - mse: 85.0015\n",
      "Epoch 316/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 85.1142 - mse: 85.1142\n",
      "Epoch 317/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 85.3524 - mse: 85.3524\n",
      "Epoch 318/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 84.9326 - mse: 84.932 - 0s 140us/step - loss: 84.7800 - mse: 84.7800\n",
      "Epoch 319/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 84.3976 - mse: 84.3976\n",
      "Epoch 320/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 80.4828 - mse: 80.482 - 0s 180us/step - loss: 85.1337 - mse: 85.1337\n",
      "Epoch 321/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 84.9513 - mse: 84.9513\n",
      "Epoch 322/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 84.6219 - mse: 84.6219\n",
      "Epoch 323/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 85.1287 - mse: 85.1287\n",
      "Epoch 324/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 84.2545 - mse: 84.2545\n",
      "Epoch 325/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 85.0069 - mse: 85.0069\n",
      "Epoch 326/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 85.0703 - mse: 85.0703\n",
      "Epoch 327/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 84.4903 - mse: 84.4903\n",
      "Epoch 328/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 85.6514 - mse: 85.6514\n",
      "Epoch 329/5000\n",
      "506/506 [==============================] - 0s 111us/step - loss: 84.7844 - mse: 84.7844\n",
      "Epoch 330/5000\n",
      "506/506 [==============================] - 0s 215us/step - loss: 84.7146 - mse: 84.7146\n",
      "Epoch 331/5000\n",
      "506/506 [==============================] - 0s 291us/step - loss: 85.0909 - mse: 85.0909\n",
      "Epoch 332/5000\n",
      "506/506 [==============================] - 0s 154us/step - loss: 84.5140 - mse: 84.5140\n",
      "Epoch 333/5000\n",
      "506/506 [==============================] - 0s 200us/step - loss: 84.7464 - mse: 84.7464\n",
      "Epoch 334/5000\n",
      "506/506 [==============================] - 0s 281us/step - loss: 84.4495 - mse: 84.4495\n",
      "Epoch 335/5000\n",
      "506/506 [==============================] - 0s 251us/step - loss: 84.5144 - mse: 84.5144\n",
      "Epoch 336/5000\n",
      "506/506 [==============================] - 0s 166us/step - loss: 84.8587 - mse: 84.8587\n",
      "Epoch 337/5000\n",
      "506/506 [==============================] - 0s 219us/step - loss: 84.7981 - mse: 84.7981\n",
      "Epoch 338/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 84.4517 - mse: 84.4517\n",
      "Epoch 339/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 84.7839 - mse: 84.7839\n",
      "Epoch 340/5000\n",
      "506/506 [==============================] - 0s 176us/step - loss: 85.3112 - mse: 85.3112\n",
      "Epoch 341/5000\n",
      "506/506 [==============================] - 0s 200us/step - loss: 84.9498 - mse: 84.9498\n",
      "Epoch 342/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 84.9251 - mse: 84.9251\n",
      "Epoch 343/5000\n",
      "506/506 [==============================] - 0s 322us/step - loss: 84.5951 - mse: 84.5951\n",
      "Epoch 344/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 84.8119 - mse: 84.8119\n",
      "Epoch 345/5000\n",
      "506/506 [==============================] - 0s 213us/step - loss: 85.1783 - mse: 85.1783\n",
      "Epoch 346/5000\n",
      "506/506 [==============================] - 0s 217us/step - loss: 85.6645 - mse: 85.6645\n",
      "Epoch 347/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 84.5988 - mse: 84.5988 0s - loss: 85.4934 - mse: 85.493\n",
      "Epoch 348/5000\n",
      "506/506 [==============================] - 0s 174us/step - loss: 84.9239 - mse: 84.9239\n",
      "Epoch 349/5000\n",
      "506/506 [==============================] - 0s 162us/step - loss: 84.8321 - mse: 84.8321\n",
      "Epoch 350/5000\n",
      "506/506 [==============================] - 0s 265us/step - loss: 84.9427 - mse: 84.9427\n",
      "Epoch 351/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 86.7818 - mse: 86.781 - 0s 213us/step - loss: 85.4249 - mse: 85.4249\n",
      "Epoch 352/5000\n",
      "506/506 [==============================] - 0s 231us/step - loss: 84.9637 - mse: 84.9637\n",
      "Epoch 353/5000\n",
      "506/506 [==============================] - 0s 237us/step - loss: 85.1331 - mse: 85.1331\n",
      "Epoch 354/5000\n",
      "506/506 [==============================] - 0s 229us/step - loss: 85.6710 - mse: 85.6710\n",
      "Epoch 355/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 85.5424 - mse: 85.5424\n",
      "Epoch 356/5000\n",
      "506/506 [==============================] - 0s 229us/step - loss: 84.9468 - mse: 84.9468\n",
      "Epoch 357/5000\n",
      "506/506 [==============================] - 0s 781us/step - loss: 84.7249 - mse: 84.7249\n",
      "Epoch 358/5000\n",
      "506/506 [==============================] - 0s 229us/step - loss: 85.1568 - mse: 85.1568\n",
      "Epoch 359/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 84.6971 - mse: 84.6971\n",
      "Epoch 360/5000\n",
      "506/506 [==============================] - 0s 170us/step - loss: 85.3031 - mse: 85.3031\n",
      "Epoch 361/5000\n",
      "506/506 [==============================] - 0s 504us/step - loss: 85.2717 - mse: 85.2717 0s - loss: 79.8654 - mse: 79.865\n",
      "Epoch 362/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 84.7367 - mse: 84.7367\n",
      "Epoch 363/5000\n",
      "506/506 [==============================] - 0s 461us/step - loss: 84.9012 - mse: 84.9012\n",
      "Epoch 364/5000\n",
      "506/506 [==============================] - 0s 194us/step - loss: 85.3549 - mse: 85.3549\n",
      "Epoch 365/5000\n",
      "506/506 [==============================] - 0s 281us/step - loss: 84.6303 - mse: 84.6303\n",
      "Epoch 366/5000\n",
      "506/506 [==============================] - 0s 206us/step - loss: 84.9050 - mse: 84.9050\n",
      "Epoch 367/5000\n",
      "506/506 [==============================] - 0s 115us/step - loss: 85.0699 - mse: 85.0699\n",
      "Epoch 368/5000\n",
      "506/506 [==============================] - 0s 134us/step - loss: 85.0085 - mse: 85.0085\n",
      "Epoch 369/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 84.6125 - mse: 84.6125\n",
      "Epoch 370/5000\n",
      "506/506 [==============================] - 0s 385us/step - loss: 85.0923 - mse: 85.0923\n",
      "Epoch 371/5000\n",
      "506/506 [==============================] - 0s 348us/step - loss: 84.9027 - mse: 84.9027\n",
      "Epoch 372/5000\n",
      "506/506 [==============================] - 0s 219us/step - loss: 84.7520 - mse: 84.7520\n",
      "Epoch 373/5000\n",
      "506/506 [==============================] - 0s 227us/step - loss: 85.4353 - mse: 85.4353\n",
      "Epoch 374/5000\n",
      "506/506 [==============================] - 0s 310us/step - loss: 85.0326 - mse: 85.0326\n",
      "Epoch 375/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 85.2741 - mse: 85.2742\n",
      "Epoch 376/5000\n",
      "506/506 [==============================] - 0s 180us/step - loss: 84.7843 - mse: 84.7843\n",
      "Epoch 377/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 84.9977 - mse: 84.9977\n",
      "Epoch 378/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 85.1493 - mse: 85.1493\n",
      "Epoch 379/5000\n",
      "506/506 [==============================] - 0s 233us/step - loss: 85.0174 - mse: 85.0174\n",
      "Epoch 380/5000\n",
      "506/506 [==============================] - 0s 213us/step - loss: 85.0446 - mse: 85.0446\n",
      "Epoch 381/5000\n",
      "506/506 [==============================] - 0s 198us/step - loss: 84.9432 - mse: 84.9432\n",
      "Epoch 382/5000\n",
      "506/506 [==============================] - 0s 306us/step - loss: 85.2817 - mse: 85.2818\n",
      "Epoch 383/5000\n",
      "506/506 [==============================] - 0s 320us/step - loss: 85.5758 - mse: 85.5758\n",
      "Epoch 384/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 84.9170 - mse: 84.9170\n",
      "Epoch 385/5000\n",
      "506/506 [==============================] - 0s 119us/step - loss: 85.1623 - mse: 85.1623\n",
      "Epoch 386/5000\n",
      "506/506 [==============================] - 0s 340us/step - loss: 84.7149 - mse: 84.7149\n",
      "Epoch 387/5000\n",
      "506/506 [==============================] - 0s 293us/step - loss: 84.7962 - mse: 84.7962\n",
      "Epoch 388/5000\n",
      "506/506 [==============================] - 0s 340us/step - loss: 84.7871 - mse: 84.7871\n",
      "Epoch 389/5000\n",
      "506/506 [==============================] - 0s 259us/step - loss: 84.7020 - mse: 84.7020\n",
      "Epoch 390/5000\n",
      "506/506 [==============================] - 0s 291us/step - loss: 84.9186 - mse: 84.9186\n",
      "Epoch 391/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 200us/step - loss: 84.9419 - mse: 84.9419\n",
      "Epoch 392/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 85.1006 - mse: 85.1006\n",
      "Epoch 393/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 85.0953 - mse: 85.0953\n",
      "Epoch 394/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 84.8499 - mse: 84.8499\n",
      "Epoch 395/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 85.0009 - mse: 85.0009\n",
      "Epoch 396/5000\n",
      "506/506 [==============================] - 0s 138us/step - loss: 85.3548 - mse: 85.3548\n",
      "Epoch 397/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 84.8531 - mse: 84.8531\n",
      "Epoch 398/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 84.8729 - mse: 84.8729\n",
      "Epoch 399/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 85.4809 - mse: 85.4809\n",
      "Epoch 400/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 84.8088 - mse: 84.8088\n",
      "Epoch 401/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 85.0897 - mse: 85.0897\n",
      "Epoch 402/5000\n",
      "506/506 [==============================] - 0s 117us/step - loss: 84.6838 - mse: 84.6838\n",
      "Epoch 403/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 85.0058 - mse: 85.0058\n",
      "Epoch 404/5000\n",
      "506/506 [==============================] - 0s 113us/step - loss: 84.6287 - mse: 84.6287\n",
      "Epoch 405/5000\n",
      "506/506 [==============================] - 0s 184us/step - loss: 85.1198 - mse: 85.1198\n",
      "Epoch 406/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 84.6555 - mse: 84.6555\n",
      "Epoch 407/5000\n",
      "506/506 [==============================] - 0s 97us/step - loss: 84.8520 - mse: 84.8520\n",
      "Epoch 408/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 84.7054 - mse: 84.7054\n",
      "Epoch 409/5000\n",
      "506/506 [==============================] - 0s 152us/step - loss: 84.8038 - mse: 84.8038\n",
      "Epoch 410/5000\n",
      "506/506 [==============================] - 0s 130us/step - loss: 85.2938 - mse: 85.2938\n",
      "Epoch 411/5000\n",
      "506/506 [==============================] - 0s 125us/step - loss: 84.9095 - mse: 84.9095\n",
      "Epoch 412/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 85.0264 - mse: 85.0264\n",
      "Epoch 413/5000\n",
      "506/506 [==============================] - 0s 144us/step - loss: 85.0633 - mse: 85.0633\n",
      "Epoch 414/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 85.0529 - mse: 85.0529\n",
      "Epoch 415/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 85.0102 - mse: 85.0102\n",
      "Epoch 416/5000\n",
      "506/506 [==============================] - 0s 117us/step - loss: 84.7085 - mse: 84.7085\n",
      "Epoch 417/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 84.9857 - mse: 84.9857\n",
      "Epoch 418/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 84.6063 - mse: 84.6063\n",
      "Epoch 419/5000\n",
      "506/506 [==============================] - 0s 215us/step - loss: 84.7838 - mse: 84.7838\n",
      "Epoch 420/5000\n",
      "506/506 [==============================] - 0s 117us/step - loss: 84.5718 - mse: 84.5718\n",
      "Epoch 421/5000\n",
      "506/506 [==============================] - 0s 146us/step - loss: 84.8722 - mse: 84.8722\n",
      "Epoch 422/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 84.9431 - mse: 84.9431\n",
      "Epoch 423/5000\n",
      "506/506 [==============================] - 0s 95us/step - loss: 84.6029 - mse: 84.6029\n",
      "Epoch 424/5000\n",
      "506/506 [==============================] - 0s 105us/step - loss: 85.1385 - mse: 85.1385\n",
      "Epoch 425/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 85.2328 - mse: 85.2328\n",
      "Epoch 426/5000\n",
      "506/506 [==============================] - 0s 182us/step - loss: 84.6507 - mse: 84.6507\n",
      "Epoch 427/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 84.8813 - mse: 84.8813\n",
      "Epoch 428/5000\n",
      "506/506 [==============================] - 0s 119us/step - loss: 85.0324 - mse: 85.0324\n",
      "Epoch 429/5000\n",
      "506/506 [==============================] - 0s 208us/step - loss: 85.0302 - mse: 85.0302\n",
      "Epoch 430/5000\n",
      "506/506 [==============================] - 0s 289us/step - loss: 84.7190 - mse: 84.7190\n",
      "Epoch 431/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 85.3960 - mse: 85.3960\n",
      "Epoch 432/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 85.3577 - mse: 85.3577\n",
      "Epoch 433/5000\n",
      "506/506 [==============================] - 0s 142us/step - loss: 84.9116 - mse: 84.9116\n",
      "Epoch 434/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 84.5612 - mse: 84.5612\n",
      "Epoch 435/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 84.8705 - mse: 84.8705\n",
      "Epoch 436/5000\n",
      "506/506 [==============================] - 0s 188us/step - loss: 85.1743 - mse: 85.1743\n",
      "Epoch 437/5000\n",
      "506/506 [==============================] - 0s 101us/step - loss: 84.3274 - mse: 84.3274\n",
      "Epoch 438/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 85.3002 - mse: 85.3002\n",
      "Epoch 439/5000\n",
      "506/506 [==============================] - 0s 150us/step - loss: 84.3784 - mse: 84.3785\n",
      "Epoch 440/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 85.3444 - mse: 85.3444\n",
      "Epoch 441/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 86.8940 - mse: 86.894 - 0s 265us/step - loss: 85.1669 - mse: 85.1669\n",
      "Epoch 442/5000\n",
      "506/506 [==============================] - 0s 249us/step - loss: 84.9922 - mse: 84.9922\n",
      "Epoch 443/5000\n",
      "506/506 [==============================] - 0s 196us/step - loss: 84.9354 - mse: 84.9354\n",
      "Epoch 444/5000\n",
      "506/506 [==============================] - 0s 265us/step - loss: 85.0329 - mse: 85.0330\n",
      "Epoch 445/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 84.7903 - mse: 84.7903\n",
      "Epoch 446/5000\n",
      "506/506 [==============================] - 0s 190us/step - loss: 84.7243 - mse: 84.7243\n",
      "Epoch 447/5000\n",
      "506/506 [==============================] - 0s 164us/step - loss: 85.0879 - mse: 85.0879\n",
      "Epoch 448/5000\n",
      "506/506 [==============================] - 0s 186us/step - loss: 85.2239 - mse: 85.2239\n",
      "Epoch 449/5000\n",
      "506/506 [==============================] - 0s 257us/step - loss: 85.2455 - mse: 85.2454\n",
      "Epoch 450/5000\n",
      "506/506 [==============================] - 0s 283us/step - loss: 85.3102 - mse: 85.3102\n",
      "Epoch 451/5000\n",
      "506/506 [==============================] - 0s 281us/step - loss: 84.5616 - mse: 84.5615\n",
      "Epoch 452/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 86.8445 - mse: 86.844 - 0s 176us/step - loss: 85.5107 - mse: 85.5107\n",
      "Epoch 453/5000\n",
      "506/506 [==============================] - 0s 178us/step - loss: 84.6896 - mse: 84.6896\n",
      "Epoch 454/5000\n",
      "506/506 [==============================] - 0s 162us/step - loss: 84.6461 - mse: 84.6461\n",
      "Epoch 455/5000\n",
      "506/506 [==============================] - ETA: 0s - loss: 83.6936 - mse: 83.693 - 0s 160us/step - loss: 84.9727 - mse: 84.9727\n",
      "Epoch 456/5000\n",
      "506/506 [==============================] - 0s 168us/step - loss: 84.6059 - mse: 84.6059\n",
      "Epoch 457/5000\n",
      "506/506 [==============================] - 0s 192us/step - loss: 84.7904 - mse: 84.7904\n",
      "Epoch 458/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 84.8885 - mse: 84.8885\n",
      "Epoch 459/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 84.5579 - mse: 84.5580\n",
      "Epoch 460/5000\n",
      "506/506 [==============================] - 0s 209us/step - loss: 84.9318 - mse: 84.9318\n",
      "Epoch 461/5000\n",
      "506/506 [==============================] - 0s 93us/step - loss: 84.8752 - mse: 84.8752\n",
      "Epoch 462/5000\n",
      "506/506 [==============================] - 0s 206us/step - loss: 84.8765 - mse: 84.8765\n",
      "Epoch 463/5000\n",
      "506/506 [==============================] - 0s 219us/step - loss: 84.8400 - mse: 84.8400\n",
      "Epoch 464/5000\n",
      "506/506 [==============================] - 0s 202us/step - loss: 85.0356 - mse: 85.0356\n",
      "Epoch 465/5000\n",
      "506/506 [==============================] - 0s 237us/step - loss: 84.9130 - mse: 84.9130\n",
      "Epoch 466/5000\n",
      "506/506 [==============================] - 0s 160us/step - loss: 85.0640 - mse: 85.0640\n",
      "Epoch 467/5000\n",
      "506/506 [==============================] - 0s 219us/step - loss: 84.6145 - mse: 84.6145\n",
      "Epoch 468/5000\n",
      "506/506 [==============================] - 0s 261us/step - loss: 84.8174 - mse: 84.8175\n",
      "Epoch 469/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 247us/step - loss: 85.6422 - mse: 85.6422\n",
      "Epoch 470/5000\n",
      "506/506 [==============================] - 0s 140us/step - loss: 84.9921 - mse: 84.9921\n",
      "Epoch 471/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 85.7303 - mse: 85.7303\n",
      "Epoch 472/5000\n",
      "506/506 [==============================] - 0s 121us/step - loss: 85.2727 - mse: 85.2727\n",
      "Epoch 473/5000\n",
      "506/506 [==============================] - 0s 136us/step - loss: 85.2612 - mse: 85.2612\n",
      "Epoch 474/5000\n",
      "506/506 [==============================] - 0s 109us/step - loss: 85.2132 - mse: 85.2132\n",
      "Epoch 475/5000\n",
      "506/506 [==============================] - 0s 107us/step - loss: 84.9675 - mse: 84.9675\n",
      "Epoch 476/5000\n",
      "506/506 [==============================] - 0s 99us/step - loss: 85.2050 - mse: 85.2050\n",
      "Epoch 477/5000\n",
      "506/506 [==============================] - 0s 128us/step - loss: 84.6212 - mse: 84.6213\n",
      "Epoch 478/5000\n",
      "506/506 [==============================] - 0s 103us/step - loss: 84.7193 - mse: 84.7193\n",
      "Epoch 479/5000\n",
      "506/506 [==============================] - 0s 156us/step - loss: 85.3401 - mse: 85.3401\n",
      "Epoch 480/5000\n",
      "506/506 [==============================] - 0s 115us/step - loss: 84.5479 - mse: 84.5479\n",
      "Epoch 481/5000\n",
      "506/506 [==============================] - 0s 132us/step - loss: 84.7810 - mse: 84.7810\n",
      "Epoch 482/5000\n",
      "352/506 [===================>..........] - ETA: 0s - loss: 87.1050 - mse: 87.105 - ETA: 0s - loss: 91.9724 - mse: 91.9724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-46d3f58551a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mbatch_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mbatch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m    365\u001b[0m         \u001b[1;31m# For backwards compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_, y_, epochs=5000, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
