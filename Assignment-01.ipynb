{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2020年1月05日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 作业截止时间\n",
    "此次作业截止时间为 2020.01.12日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：每道题是否回答完整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {1.特斯拉自动驾驶系统 2.ALPHAGO围棋对战程序 3.苹果手机的Siri可以和人聊天}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {Github 可以作为免费的远程仓库，也可以学习和借鉴别人的优秀成果。\n",
    "     Jupyter集成整个分析过程，并将说明文字、代码、图表、公式、结论都整合在一个文档中，所以需要使用。\n",
    "     PyCharm与IPython Notebook集成，提供交互式Python控制台， 并且支持Anaconda和多种科学化的包（例如matplotlib和NumPy）,所以需要使用}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:基于概率的语言模型，判断的是计算机生成的一句话到底是正确还是错误，需要采用一个大量的文本库，计算每一句话的概率，概率较高者正确的可能性更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:搜狗拼音输入法，输入一个句子的拼音，计算机会给出一个最可能的输入排列。谷歌的自动翻译系统，也是基于概率统计，计算不同翻译句子的概率高低，给出最合适的翻译结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:在日常工作中，我们使用AI生成句子，需要对句子是否正确进行判别，就要用到概率模型。在实际工作中，65%的工作量都在文字的预处理，其中难点主要有，首先确定文字的编码，然后过滤掉无意义的字符和正则表达式，如“换行，\\&?#!等”，然后调用jieba库进行分词处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:AI在处理语言问题时采用的解决思路，包括语法树和概率模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:机器翻译，语音识别都需要用到语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:它的第一个特点是某个词的出现依赖于其他若干个词，第二个特点是我们获得的信息越多，预测越准确。判断句子概率时，注意到第一个单词和紧邻的一个单词的关联度更高，第一个单词和一本书的最后一个单词关联度很低，所以计算一个句子中每一个单词和后面相邻单词的概率，从而计算出句子的相应概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:N-gram模型的优点在于它包含了前N-1个词所能提供的全部信息，这些词对于当前词的出现具有很强的约束力，然而它的缺点是需要相当规模的训练文本来确定模型的参数。\n",
    "$$ Pr(Sentence)=Pr(W_1W_2W_3W_4)=\\prod_i^{n}\\frac{\\#W_iW_i+1}{\\#W_{i+1}}* Pr(W_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:2-gram指一个由2个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "# 例子：俄空军在叙利亚领空执行任务的地区里发现可疑飞行物\n",
    "find_something = \"\"\"\n",
    "find_something = who at where find something\n",
    "who = 国家 部门\n",
    "国家 = 美国|俄罗斯| 中国 |日本 |韩国\n",
    "部门 = 国土安全局 | 食品药品监督局 |警署 |太空总署 |警备厅 | 海洋局\n",
    "at = 在\n",
    "where = 月球表面 | 地球表面 |叙利亚领空 | 日本| 韩国| 海洋深处| 家里| 出租屋中\n",
    "find= 发现 | 找到\n",
    "somethign= adj none\n",
    "adj= 可疑的| 有趣的|开心的| 巨大的| 令人恐惧的| 快乐的\n",
    "something= 外星人| 笔记本| 手机| 炸弹| 鲸鱼| 火星车| 杯子 |飞行物\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否提出了和课程上区别较大的语法结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "# 例子：玉山坚持每月上门看望徐满生\n",
    "look_after = \"\"\"\n",
    "look_after=who insist howofen  lookafter somebody\n",
    "who=玉山| 李德华| 李嘉诚| 王祖贤|徐满生\n",
    "insist=坚持| 一直\n",
    "howofen=每月|每年|每天|每分钟\n",
    "lookafter=看望|拜访\n",
    "somebody= 病人| 白血病儿童|警察叔叔| 不良少年| 加班族\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**：是否和上一个语法区别比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_str: ['美国太空总署在韩国找到笔记本', '韩国国土安全局在叙利亚领空发现杯子', '日本食品药品监督局在韩国发现外星人', '中国海洋局在叙利亚领空发现飞行物', '美国太空总署在叙利亚领空找到手机', '俄罗斯海洋局在日本找到飞行物', '美国警署在地球表面找到杯子', '俄罗斯海洋局在日本发现手机', '韩国海洋局在出租屋中找到笔记本', '中国国土安全局在韩国找到外星人']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "    # you code here \n",
    "def generate_grammar(grammar_str:str,target ,split='='):\n",
    "    grammar={}\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "        expression,formula = line.split(split)\n",
    "        formulas=formula.split('|')\n",
    "        formulas=[f.split() for f in formulas]\n",
    "        grammar[expression.strip()]=formulas\n",
    "        #print('expression,formulas:',expression,formulas)\n",
    "    return grammar\n",
    "choice_a_expr=random.choice\n",
    "def generate_by_grammar(grammar:dict,target:str):\n",
    "    if target not in grammar:return target\n",
    "    # the above line is to test if target is a key\n",
    "    expr = choice_a_expr(grammar[target])\n",
    "    #print('expr:',expr)\n",
    "    return ''.join(generate_by_grammar(grammar,t)for t in expr)\n",
    "\n",
    "def generate(grammar_str,split,target):\n",
    "    grammar=generate_grammar(grammar_str,target,split)\n",
    "    return generate_by_grammar(grammar,target)\n",
    "def generate_n(grammar_str,target_str):\n",
    "    output_str=[]\n",
    "    for i in range(10):\n",
    "        #print           (generate(grammar_str,split=\"=\",target=target_str))\n",
    "        output_str.append(generate(grammar_str,split='=',target=target_str))\n",
    "    return output_str\n",
    "        #return(generate(grammar_str,split=\"=\",target=target_str))\n",
    "    #for i in range(10):\n",
    "    #    print(generate(find_something,split=\"=\",target='find_something'))\n",
    "generated_sentences=[]\n",
    "generated_sentences+=generate_n(find_something,'find_something')\n",
    "print('output_str:',generated_sentences)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**; 运行代码，观察是否能够生成多个句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles total: 261497\n",
      "len of articles_clean 261497\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#random.choice(range(100))\n",
    "filename = 'movie_comments.csv'\n",
    "import pandas as pd\n",
    "content = pd.read_csv(filename, encoding='utf-8') ##gb18030\n",
    "articles = content['comment'].tolist()\n",
    "print('articles total:',len(articles))\n",
    "def cut(string): return jieba.cut(string)\n",
    "import jieba\n",
    "#print(list(jieba.cut('这个是用来做汉语分词的')))\n",
    "from collections import Counter\n",
    "#with_jieba_cut = Counter(jieba.cut(articles[110]))\n",
    "#with_jieba_cut.most_common()[:10]\n",
    "''.join(token(articles[110]))\n",
    "\n",
    "import re  #需要使用正则表达式\n",
    "def token(string):\n",
    "    # we will learn the regular expression next course.\n",
    "    return re.findall('\\w+', string)  #所有的Words\n",
    "\n",
    "\n",
    "articles_clean = [''.join(token(str(a)))for a in articles]\n",
    "print('len of articles_clean',len(articles_clean))\n",
    "\n",
    "with open('article_movie.txt', 'w',encoding='utf-8') as f:\n",
    "    for a in articles_clean:\n",
    "        f.write(a + '\\n')\n",
    "\n",
    "import jieba\n",
    "def cut(string): return jieba.cut(string)\n",
    "ALL_TOKEN=cut(open('article_movie.txt',encoding='utf-8').read())\n",
    "TOKEN=[]\n",
    "for i, t in enumerate(ALL_TOKEN):\n",
    "    #if i>50000:break\n",
    "    if i>200000:break\n",
    "    #大家把它变成200000\n",
    "    #if i % 10000==0: print(i)\n",
    "    TOKEN.append(t)\n",
    "\n",
    "from collections import Counter\n",
    "words_count = Counter(TOKEN)\n",
    "words_count.most_common(100)\n",
    "frequiences = [f for w, f in words_count.most_common(100)]\n",
    "x = [i for i in range(100)]\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(x, frequiences)\n",
    "import numpy as np\n",
    "#plt.plot(x, np.log(frequiences))\n",
    "def prob_1(word):\n",
    "    return words_count[word] / len(TOKEN)\n",
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "def prob_1(word): return words_count[word] / len(TOKEN)\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2: \n",
    "        return words_count_2[word1+word2] / words_count[word2]\n",
    "    else:\n",
    "        return 1/len(words_count)  #out of vocabulary lib\n",
    "\n",
    "def get_probablity(sentence):\n",
    "    words = list(cut(sentence))\n",
    "    \n",
    "    sentence_pro = 1\n",
    "    \n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        \n",
    "        probability = prob_2(word, next_)\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "    sentence_pro *= prob_1(words[-1])\n",
    "    return sentence_pro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点** 1. 是否使用了新的数据集； 2. csv(txt)数据是否正确解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(): # you code here\n",
    "    generated_sentences=[]\n",
    "    generated_sentences+=generate_n(look_after,'look_after')\n",
    "    #print('output_str:',generated_sentences)\n",
    "    #print('length is:',len(generated_sentences))\n",
    "    len_sen=len(generated_sentences)\n",
    "    posb_senten=[]\n",
    "    print(\"begin get_probality of each sentence!\")\n",
    "    for i in range(len_sen):\n",
    "        #print(i)\n",
    "        posb_senten.append([generated_sentences[i],get_probablity(generated_sentences[i])])\n",
    "        #print(posb_senten[i])\n",
    "    return posb_senten\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin get_probality of each sentence!\n",
      "10\n",
      "best sentence is: ['李嘉诚坚持每年看望警察叔叔', 3.635433085727854e-27]\n"
     ]
    }
   ],
   "source": [
    "res_posb_senten=[]\n",
    "res_posb_senten+=generate_best()\n",
    "print(len(res_posb_senten))\n",
    "a=sorted([res_posb_senten[1],res_posb_senten[2],res_posb_senten[3],res_posb_senten[4],res_posb_senten[5],res_posb_senten[6],res_posb_senten[7],res_posb_senten[8],res_posb_senten[9],res_posb_senten[0]], key=lambda x: x[1], reverse=True)\n",
    "print(\"best sentence is:\",a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **评阅点**： 是否使用 lambda 语法进行排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:语言结构树比较简单，生成的语句还不是很合理;采用的数据量只有20万，还有提升空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**评阅点**: 是否提出了比较实际的问题，例如OOV问题，例如数据量，例如变成 3-gram问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
